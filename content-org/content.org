#+STARTUP: overview
#+HUGO_BASE_DIR: ~/projects/chifan/sites/wych42.github.io
#+SEQ_TODO: TODO DRAFT DONE
#+HUGO_AUTO_SET_LASTMOD: t
#+hugo_locale: zh
#+property: header-args :eval never-export
#+author: 
#+hugo_custom_front_matter: :author "chi"

* Pages
:PROPERTIES:
:EXPORT_HUGO_SECTION: cn
:END:

** DONE Blog Archive
   CLOSED: [2018-10-25 Thu 12:37]
:PROPERTIES:
:EXPORT_FILE_NAME: archive.md
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :type archive :comment false
:END:


* Posts
:PROPERTIES:
:EXPORT_HUGO_SECTION: cn/post
:END:

** DRAFT 企业微信远程打卡                                                       :@Post:wechat:work:
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: wechat-punch
   :EXPORT_FILE_NAME: index
   :EXPORT_DATE: [2018-04-23 Thu 13:13]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc false
   :END:

一种简单绕过定位打卡的方法。

#+hugo: more

如果你碰到了弹性工作时间却要求定位打卡又经常忘的情况,可以谨慎使用下面的方法。

***  准备

 - 安卓设备。（这里用的是 moto g 2nd，老设备发挥余热）
 - 安装好 Xposed 框架。(这里用的是 LineageOS 13 刷 Xposed ARM)
 - 安装 [[https://www.coolapk.com/apk/com.rong.xposed.fakelocation][模拟位置]] 模块。 安装完成后，去 Xposed 里启用模块，重启手机使之生效。
 - 安装好企业微信/钉钉。
 - 可选：PC 安装好 adb。

 *有动手能力的读者，如果有越狱的 iOS 设备，也可以做同样的事情*

***  模拟位置设置

 打开「模拟位置」，看到应用列表，点击需要被模拟位置的。

 [[file:static/wechat-punch/fakelocation-list.png]]

 点击「地图」Icon 选择地点, 会自动填入 GPS 信息, 重新选择过地点之后，点击下「更新」按钮，通知对应的应用。

 [[file:static/wechat-punch/fakelocation-setting-app.png]]

 重点是，微信、企业微信这些应用可能不(只)使用设备的 GPS 获取位置，也可能通过基站、网络来判断位置，所以如果只设置上面的地点不生效，可以填一下模拟基站信息。把设备带到正确的地点，在拨号盘输入 `*#*#4636#*#*`（Android 有效），在「手机信息」里，查到 mMNC, mCID, mTAC 几项填入保存。再重启需要模拟定位的应用即可。

 好用的 GPS、基站工具：[[http://www.gpsspg.com/][GPSspg]], [[http://www.cellocation.com/][cellocation 坐标反查基站接口]]

** DONE beancount 简易入门指南                                                  :@Post:beancount:记账:
   CLOSED: [2018-10-25 Thu 12:25]
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: beancount-intro
   :EXPORT_FILE_NAME: index
   :EXPORT_DATE: [2018-10-23 Tue 12:46]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

beancount 是一个基于文本、命令行的复式记账软件，上周看到了 [[https://wzyboy.im/post/1063.html][wzyboy]] 介绍这个工具以及复式记账的基础概念的安利文。花了一点时间入坑之后，自己的资产（存款）、收入、消费去向，一目了然。看到食物消费和打车话费在支出的 TreeMap 里占据的两大块，不禁让我反思起平时的好吃懒做。

#+hugo: more

如果你跟我类似：

- 靠月工资流水生存，有强烈的意愿、需求理清自己的财务状态，搞清楚钱从哪里来、花到哪里去、留下了多少，并借机改善；
- 虽然平时也用过鲨鱼记账、随手记之类的 App，但是觉得每一次消费后掏出手机、打开 App、填写金额时间用途，是一个很重的中断行为；也懒得定期手工录入一批账单；
- 绝大多数消费最终都由银行账户结算，例如通过支付宝绑定信用卡快捷支付，最终只需要统计银行账单即可；
- 懂一点编程（简单的 Python 基础即可），愿意付出一点时间学习；
  
那么，beancount 这个工具会是一个比较合适的开始。

本文将从一个新手了解了基础概念之后，如何迈出第一步，把现存的财务状态映射到 beancount 里的角度，介绍一下个人的实践经验。

*** 背景知识

后文假设读者已经读过下列文档、或者熟悉下面文档中提及的工具和概念。

- [[https://wzyboy.im/post/1063.html][Beancount —— 命令行复式簿记]] wzyboy 的安利文，中文文档

如果有时间，推荐再读一下 beancount 作者写的几篇文档：

- [[https://docs.google.com/document/d/100tGcA4blh6KSXPRGCZpUlyxaRUwFHEvnz_k9DyZFn4/edit][Beancount - The Double-Entry Counting Method]]
- [[https://docs.google.com/document/d/1G-gsmwK551lSyuHboVLW3xbLhh99JfoKIbNnZSJxteE/edit][Beancount - Tutorial & Example]]

还有更多的细节、示例可以参考这个索引[[https://docs.google.com/document/d/1RaondTJCS_IUPBHFNdT8oqFKJjVJDsfsn6JEjBG04eA/edit][文档]]。

*** 目录结构

使用如下所示的目录结构：

#+BEGIN_SRC
~/Documents/accounting
├── documents
│   ├── Assets/
│   ├── Expenses/
│   ├── Income/
│   └── Liabilities/
├── documents.tmp/
├── importers
│   ├── __init__.py
├── yc.bean
└── yc.import
#+END_SRC

- ~/Documents/accounting: 项目的根目录，放在任何想放的地方;
- documents: 用于导入第三方数据后，使用 bean-file 归类存档原文件，执行 ~mkdir documents/{Assets,Expenses,Inconme,Liabilities}~ 创建这些目录;
- documents.tmp: 用于临时存放待导入的第三方数据，比如银行卡账单，可以是其他路径，比如 ~/tmp~, bean-extract, bean-file 这些脚本都要用到这个目录;
- importers: 用于存放自定义的导入脚本，目前自带的导入器足够使用，可以先不管这个目录;
- yc.import: 实际上是一个 Python 脚本，用于定义导入器的配置，后面细说;
- yc.bean: 是实际的账簿文件，账户、交易记录都在这里，生成报表也使用这个文件,可以将账簿文件拆分成多个小文件，再使用 ~include~ 指令拼接，类似 C 语言或者 Python 里的 ~import~;

#+begin_details
#+begin_summary
单文件账簿还是拆分多文件账簿？
#+end_summary
- 刚开始建议用一个 ~.bean~ 文件管理所有的记录，熟悉工具的使用流程、有了明确的需求之后再拆分;
- 如果使用 emacs 的 orgmode 编辑账簿文件，建议一直使用一个 ~.bean~ 文件，非常好用;
#+end_details

刚开始使用，只需要关注主账簿文件 ~yc.bean~ 就行，我们来一探究竟吧。

*** 开设账户

我的 ~yc.bean~ 文件顶层有三部分: Options, Accounts, MonthlyReconciliation，分别对应账簿文件的选项，账户，每月对账。

**** Options

设置账簿的 title，定义账簿里会用到的货币。

 #+BEGIN_SRC
\* Options
option "title" "My Personal Ledger"
option "operating_currency" "CNY"
option "operating_currency" "USD"
#+END_SRC

**** Accounts
有五种账户类型: Assets,Liabilities,Equity,Income,Expenses。分别对应资产、负债、初始化账簿时已有的数据、收入、支出，详细含义可以看上面提及的推荐阅读文档里。

在 benacount 里会隐式创建树形账户，也就是如果开了一个账户叫做： ~Assets:Bank:BoC:CardXXXX~, 那么会自动生成账户 ~Assets:Bank:BoC~, ~Assets:Bank~, ~Assets~ 。我的做法是原则上用现实世界里的最细分的账户映射 beancount 里的账户，结合账户的实际用途设置账户名。

***** 如何选择账户初始日期？

偷懒的话可以选择 1970-01-01。

我的做法是：Assets 类账户选择我开始使用 beancount 的日期，Liabilities、Expenses 账户用生日，Income 选择当前这份工作的日期。

***** Assets
 假设我在招商银行有两张储蓄卡，其中一张开通了朝朝盈的理财服务并且用于日常消费，另一张卡用于每月定额存款，积累资金用于凑购房首付，那么我会这样设置 Assets 账户(XXXX 是卡号末四位，下面同理)：

#+BEGIN_SRC
1970-01-01 open Assets:Bank:CMB:CardXXXX:Deposit CNY
1970-01-01 open Assets:Bank:CMB:CardXXXX:ZZY CNY
#+END_SRC

 对于存款卡，因为只用于特定用途，不会挪作他用，还有别的账户里也有存款用于同样的用途，比如政府的住房公积金，那么我这样设置账户：

#+BEGIN_SRC
1970-01-01 open Assets:Saving:HouseFund:Bank:CMB:CardXXXX:Deposit CNY
1970-01-01 open Assets:Saving:HouseFund:Goverment CNY
#+END_SRC

***** Liabilities

假设我在招商银行有一张银联信用卡，一张 Visa 信用卡；在交通银行有一张银联信用卡，一张 Vsia 信用卡。由于招商银行共享额度、合并账单、征信内只有一个账户；交通银行虽然也共享额度，但是拆分账单，每个账单要单独还款，并且在征信系统内一卡一账户，我这样设置账户：

#+BEGIN_SRC
1970-01-01 open Liabilities:CreditCards:CMB CNY
1970-01-01 open Liabilities:Creditcards:COMM:CardVisaXXXX CNY
1970-01-01 open Liabilities:Creditcards:COMM:CardUnionXXXX CNY
#+END_SRC

这样既可以既可以对单个账户断言 balance，也可以对单个银行对断言 balance。

***** Income

工资收入可以设置账户 Income:CompanyName:Salary 就行, 如果有饭补、报销之类的，可以单写 Income:CompanyName:FoodSubsidy, Income:CompanyName:Reimbursement.
这里用 event 指令，可以记录下哪天加入公司，比如 ~2018-01-01 event "入职 XX"~ 。

***** Expenses

基本原则同上，我在 Expenses 分类下设置了如下几种账户：

- 政府相关的：主要是五险一金、税之类。

#+BEGIN_SRC
1970-01-01 open Expenses:Government:Pension CNY
1970-01-01 open Expenses:Government:Unemployment CNY
1970-01-01 open Expenses:Government:MedicalCare CNY
1970-01-01 open Expenses:Government:IncomeTax CNY
#+END_SRC

- 日常消费，按照衣食行分了几大类，可以包含交通、食物、下馆子、日用杂物、买书、订阅（软件、VPS之类）以及宠物的支出。基本都在三级以内，再通过交易的 [[https://docs.google.com/document/d/1wAMVrKIA2qtRGmoVDSUBJGmYZSygUaR0uOMW1GV3YE0/edit#heading=h.2xx3dcvvf0r8][tag]] 标记消费的具体支出，比如食物相关的交易记录会打上这些 Tag：早餐、日常饮用水、饮料、零食等等，可以按需使用，最终在 fava 生成的网页里可以按照 tag 过滤查看。
- 住的消费相对固定，并且因为是在北京租房，也是一笔不小的支出，单独开设一类账户用来管理，建议使用当前住宿房屋的简称，比如：Expenses:Lofter0817:Rent, Expenses:Lofter0817:Utility。

***** Equity

目前我只设置了一个 Equity 账户 Equity:Opening-Balances，用来平衡初始资产、负债账户时的会计恒等式。也就是，我想往一个银行卡账户里添加 1000 元，并且想保持平衡，那么需要从某个账户减 1000 元，在初始化时，这个账户就是 Equity:Opening-Balances。一个示例：

#+BEGIN_SRC
1970-01-01 open Assets:Bank:CMB:CardXXXX CNY
1991-05-21 pad Assets:Bank:CMB:C6698 Equity:Opening-Balances
2018-10-17 balance Assets:Bank:CMB:C6698 11912.77 CNY
#+END_SRC

**** Balance

设置了账户之后，要把对应的现实账户的状态反应出来，需要用 ~balance~ 指令进行断言操作，用 ~pad~ 指令进行辅助。比如在设置账户的当时，银行卡内有存款 1000 元，可以在 ~open~ 账户那行之后添加变成下面的结构，注意 beancount 默认交易都在一天的开始发生，所以 balance 断言要写在第二天，表示截止到第二天零点的情况。

#+BEGIN_SRC
1970-01-01 open Assets:Bank:CMB:Card0817
1970-01-01 pad Assets:Bank:CMB:Card0817 Equity:Opening-Balances
1970-01-02 balance Assets:Bank:CMB:Card0817 1000 CNY
#+END_SRC

其他账户依照此方法设置即可。

*** 导入数据

除了账户和 balance 断言， ~.bean~ 文件里大部分内容是一笔笔交易记录，一个笔交易在 beancount 里一般长这样：

#+BEGIN_SRC
2018-10-22 * "描述"
  card: "CardXXXX"
  date: 2018-10-21
  Liabilities:CreditCards:CMB  -1921.00 CNY
  Expenses:Other
#+END_SRC

2018-10-22 是银行记帐日期，"*" 号表示交易确认无误，接着是交易描述；后两行是 metadata，可以用于过滤；接下来是交易涉及的账户，有减操作的账户，就有加操作的账户，这里 Expenses:Other 账户没有写加金额，是因为加操作只涉及这一个账户，beancount 会自行补齐数据。更详细的可以参考 [[https://docs.google.com/document/d/1wAMVrKIA2qtRGmoVDSUBJGmYZSygUaR0uOMW1GV3YE0/edit#][Beancount Language Syntax]] 。

每笔交易都这么手写一遍就太低效率了，还好 beancount 支持从导入第三方数据，前文提到的 ~importers~ 目录内就可以用来存放自定义的导入脚本，不过自带 csv 导入器就可以解决目前绝大部分需求。

**** 获取数据

目前国内部分银行提供 csv 各式的对账单，比如招商银行可以登录个人网银后找到对账单下载；也有银行不提供 csv、Excel 各式的对账单下载，可以尝试下面两个方法：

- 如果银行提供网页版对账单，并且账单页面内容是 html table，可以使用 Chrome 插件[[https://chrome.google.com/webstore/detail/table-capture/iebpjdmgckacbodjpijphcplhebcmeop][ Table-Capture]] 把页面里的 table 导出到 Google Spreadsheet，再导出为 csv;
- 银行应该都会提供 pdf 各式的对账单，可以尝试用 [[https://tabula.technology/][Tabula]] 这个工具，从 pdf 文件里解析账单表格并导出;
经过测试，以上两个方法能够搞定招商、交通、中信、浦发这四个银行账单。

**** 准备数据

获取到 csv 各式的数据后，需要做一些准备工作：

- 去除文件里的奇怪的符号，比如交通银行的账单里会包含 ~^M~ 这个符号，用 ~C-c C-m~ 可以在终端里敲出这个字符；
- 金额改为只保留数字部分；
- 把文件编码转换为 utf-8: ~iconv -f gbk -t UTF-8 file > file.utf-8~ ；
- 转换文件的换行方式: ~dos2unix file.utf-8~ ；

**** import 配置

我的 import 配置文件 ~yc.imoprt~ 抹去敏感信息之后示例如下下方的代码。

#+BEGIN_SRC python
#!/usr/bin/env python

import os
import sys

import beancount.ingest.extract
from beancount.ingest.importers import csv

beancount.ingest.extract.HEADER = ''

CONFIG = [
    # CMB Credit
    csv.Importer(
        {
            csv.Col.DATE: '记账日期',
            csv.Col.TXN_DATE: '交易日期',
            csv.Col.NARRATION1: '交易摘要',
            csv.Col.AMOUNT_DEBIT: '人民币金额',
            csv.Col.LAST4: '卡号后四位'
        },
        account='Liabilities:CreditCards:CMB',
        currency='CNY',
        regexps='\t对账标志',
        last4_map={
            "0000": "招行 0000",
        },
        # categorizer=guess.guess2
    ),
    # COMM Credit 0000
    csv.Importer(
        {
            csv.Col.DATE: '记账日期',
            csv.Col.TXN_DATE: '交易日期',
            csv.Col.NARRATION1: '交易说明',
            csv.Col.AMOUNT_DEBIT: '清算币种/金额',
            csv.Col.LAST4: '卡号末四位'
        },
        account='Liabilities:CreditCards:COMM:C0000',
        currency='CNY',
        regexps='交行0000',
        skip_lines=1,
        last4_map={
            "0000": "交行 0000",
        },
        # categorizer=guess.guess2]
    )
]
#+END_SRC

csv.Col.XXX 对应的是 csv 文件的 header，新加账户、账单的话对照修改就行。整体执行流程大约是，对于一个待导入文件：

1. 每个 importer 判断自己是否会处理这个文件，如果会处理，交给这个 imoprter 处理导入，并不再往下判断；csv importer 是通过 regexps 参数里指定的正则匹配整个文件内容，看能否匹配上。
2. 由于交行（其他银行也有可能）一卡一账单，账单的头部都一样，我在 csv header 下面插入一行 “交行0000”（0000是卡号末四位）来标记此文件是哪张卡的账单，应该对应到哪个账户，再配置 skip_lines 参数，在实际导入的时候跳过这一行。
3. last4_map 会匹配卡号末四位，生成 ~card: 交行 0000~ 写到交易的 metadata 里。


**** 执行导入

把准备好的账单文件放到上面提及的 documents.tmp 目录里，再执行:

#+BEGIN_SRC bash
bean-extract yc.import ${PWD}/documents.tmp > tmp.bean
#+END_SRC

我习惯先把记录先导出到临时账簿文件里，检查一下交易记录、修正一部分交易描述、添加 Expenses 账户，再导出到总账簿文件里。

添加 Expenses 账户这一步可以尝试自定义 categorizer 来实现自动化，比如交易描述里包含“饿了么”自动归到 Expenses:Food 账户里。我还没有实现这部分，可以参考这个 [[https://bitbucket.org/blais/beancount/pull-requests/24/improve-ingestimporterscsv/diff][Pull Request]]。

导入完成后，再执行下面的命令，把原文件归档到 documents 目录里。
#+BEGIN_SRC
bean-file yc.import ${PWD}/documents.tmp -o documents
#+END_SRC


*** 我的工作流

目前我的大部分支出会落到信用卡里，少量走借记卡，极少现金。信用卡出账单日也统一到一两天之内。整体工作流程大概是这样：

1. 每月最后一个账单出来后，整理好账单文件，用 bean-extract 导入账单；
2. 对 Liabilities 账户进行 balance 断言；
3. 在还款日前还款后，对 Assets 账户断言；
4. 发薪日再次对各类账户进行一次断言；
5. 每月检查个账户的错误情况，fava 生成的网页上有一个 Errors 子页面；回顾支出情况；

*** 总结

开始说要记账、规划自己的财务状况有半年多了，断断续续用过几款 App，都没有能完全坚持下来，直到在 wzyboy 的博客上看到 beancount 工具的安利，有如开挂一样，个人的财务状况从整体到细节都能看的清楚，也是我喜欢的纯文本工具，信息不会留在第三方、方便各种编辑、导入导出、备份。

在入门上手期间，通过邮件向 [[https://wzyboy.im/][wzyboy]] 请教了不少疑问，都得到了细致及时的解答，表示感谢。
** DONE 用 kubeadm 部署简易 kubernetes 集群                                     :@Post:kubernetes:
   CLOSED: [2019-12-04 Wed 11:34]
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: deploy-kubernetes-with-kubeadm
   :EXPORT_FILE_NAME: index
   :EXPORT_DATE: [2019-12-02 Mon 11:28]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

*** 准备

    - 这次部署能用到的设备都是小型号得 vps，零散在不同得公网区域，所以要部署一个跨公网集群。
    - 各项配置都使用最简化得模式，比如单主节点，主要目的是自用+测试。
    - 通读和参考文档： [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm][Bootstrapping clusters with kubeadm]]。

**** 设备和网络

     三台设备：
     - 控制节点（同时也当做worker用）: 赵云, 2 Core, 4G(x1)
     - worker: 赵云，1 Core, 1G(x2)
     - 操作系统: debian 9


     网络：三台设备在三个地区，各有公网，内网不通。

**** 安装依赖

***** 安装 docker

      #+BEGIN_SRC bash
      apt-get install -y \
              apt-transport-https \
              ca-certificates \
              curl \
              gnupg2 \
              software-properties-common

      curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -

      add-apt-repository \
          "deb [arch=amd64] https://download.docker.com/linux/debian \
         $(lsb_release -cs) \
         stable"

      apt-get update && apt-get install -y docker-ce docker-ce-cli containerd.io
      #+END_SRC

      docker 使用 systemd 作为 [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#kubeadm-blocks-waiting-for-control-plane-during-installation][cgroupdriver]]:

      #+BEGIN_SRC bash
      cat > /etc/docker/daemon.json <<EOF
      {
        "exec-opts": ["native.cgroupdriver=systemd"],
        "log-driver": "json-file",
        "log-opts": {
          "max-size": "100m"
        },
        "storage-driver": "overlay2"
      }
      EOF
      systemctl restart docker
      #+END_SRC

***** 安装 kubeadm/kubelet/kubectl

      #+BEGIN_SRC bash
      apt-get update && apt-get install -y apt-transport-https
      curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
      cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
      deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
      EOF
      apt-get update
      apt-get install -y kubelet kubeadm kubectl
      #+END_SRC

*** 部署集群

**** 初始化控制节点

      #+BEGIN_SRC bash
      kubeadm init --control-plane-endpoint=<your-endpoint-fqdn> --pod-network-cidr=192.168.0.0/16 --image-repository=registry.aliyuncs.com/google_containers  --upload-certs
      #+END_SRC

      - <your_endpoint_fqdn> 替换为集群的控制节点 IP 地址或者域名。
      - cidr 按照 flannel 的[[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network][文档]]设置。
      - 国内的机器初始化时，image repository 替换为阿里云得镜像。


      根据屏幕输出，配置好 kubeconfig
      #+BEGIN_SRC bash
      mkdir -p $HOME/.kube
      sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
      sudo chown $(id -u):$(id -g) $HOME/.kube/config
      #+END_SRC

**** 启用 flannel

     #+BEGIN_SRC bash
     sysctl net.bridge.bridge-nf-call-iptables=1
     kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
     #+END_SRC

**** worker 加入集群

     在 kubeadm init 结束时，终端输出的命令，拷贝到 worker 节点上执行。
     #+BEGIN_SRC bash
     kubeadm join k8s.xiatiao.io:6443 --token <your-token> \
    --discovery-token-ca-cert-hash sha256:<your-hash>
     #+END_SRC

     如果错过了这段输出，可以用下面得命令生成：
     #+BEGIN_SRC bash
     kubeadm token create --print-join-command
     #+END_SRC

**** 控制节点兼任 worker

     想在控制节点上也跑一些任务，需要解除限制:
     #+BEGIN_SRC bash
     kubectl taint nodes --all node-role.kubernetes.io/master-
     #+END_SRC
**** 节点打标签
     三个节点，一个设置为 forwarder， 两个设置为 backend:

     #+BEGIN_SRC bash
     kubectl lable nodes <nodename> role=<role>
     #+END_SRC
*** 善后

    至此，一个单节点的 kubernetes 集群就配置完成了，但是，由于三个设备在三个公网区域里，内网互不相同。虽然 api-server-address 是公网可用的，但是比如 =kubectl logs= 之类的命令，会尝试直接连接 worker 节点的内网地址，显然是不通的。这就导致了，虽然 pods 能被调度到各个节点上，但是集群「内部」的网络是不通的，services 就用不了。

    解决方案有几种：

    - =--advertise-address= 设置为设备的公网地址, 通过 kubeadm 执行的话，要求这个地址在设备上能看到，类似某赵云用了 elastic ip 就不行。
    - 配置 nat 转发
      + 在控制节点上: =iptables -t nat -A OUTPUT -d <worker private ip> -j DNAT --to-destination <worker public ip>=
      + 在 worker 节点上： =iptables -t nat -A OUTPUT -d <master private ip> -j DNAT --to-destination <master public ip>=
    - 用类似 wireguard, slack/nebula 之类的工具，先把各个设备组一个 overlay network，再部署 kubernetes 集群。

*** 命令备忘
    - 一键毁灭集群：
      #+BEGIN_SRC bash
      for i in $(kubectl get nodes | tail -n +2 | awk '{print $1}' ); do kubectl drain $i --delete-local-data --force --ignore-daemonsets; kubectl delete node $i; done
      kubeadm reset
      iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
      #+END_SRC
** DONE 用 slack/nebula 在公网上部署 kubernetes 集群                            :@Post:kubernetes:nebula:
   CLOSED: [2019-12-04 Wed 18:52]
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: kubernetes-cross-multiple-cloud-over-nebula
   :EXPORT_FILE_NAME: index
   :EXPORT_DATE: [2019-12-04 Wed 17:49]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

   这是[[/deploy-kubernetes-with-kubeadm/][上一篇]]的后续，顺带解决更多需求：

   - 在不同地区（云厂商）有多台设备, 甚至只在内网的设备（比如家里的 NAS）
   - 用一个 kubernetes 集群管理这些设备上的运行的部分服务, 虽然不是好的实践，但是自己用真的方便
   - 最好这些机器可以直接互联, 就像都在一个子网里一样

   看起来我需要的就是一个搭建 overlay network 的方案，而且没有公网入口（但是有出口）的设备也可加入这个网络。

   自然就想到了用过的 [[https://www.wireguard.com/quickstart/][wireguard]] 和 [[https://www.tinc-vpn.org/][tinc-vpn]]。然而，这俩配置和使用都很麻烦，也主要是用来作 vpn 的，于是就尝试下新项目 [[https://github.com/slackhq/nebula][slack/nebula]]。

*** 安装和配置 nebula

    三台设备：
    - a.host.com, 作为 lighthouse, 公网 IP 1.2.3.4, 组网 IP =192.168.100.1/24=
    - b.host.com, 组网 IP =192.168.100.2/24=
    - c.host.com, 组网 IP =192.168.100.3/24=

**** 安装 nebula

     #+BEGIN_SRC bash
     wget -qO- https://github.com/slackhq/nebula/releases/download/v1.0.0/nebula-linux-amd64.tar.gz | tar  xvf - -C /usr/local/bin
     #+END_SRC

**** 生成证书和配置文件

     #+BEGIN_SRC bash
     ./nebula-cert ca -name "calico"
     ./nebula-cert sign -name "a.host.com" -ip "192.168.100.1/24" -groups "lighthouse"
     ...
     #+END_SRC

     配置文件参考[[https://github.com/slackhq/nebula#5-configuration-files-for-each-host][文档]]设置就行，有几项需要注意：

     所有设备上：
     #+BEGIN_SRC yaml
       # 都要注意把本机对应的证书和 *ca.crt* 拷贝到配置文件里写的路径：
       pki:
         # The CAs that are accepted by this node. Must contain one or more certificates created by 'nebula-cert ca'
         ca: /etc/nebula/ca.crt
         cert: /etc/nebula/a.host.com.crt
         key: /etc/nebula/a.host.com.key
       # 把 lighthouse 的公网 IP 映射加上
       static_host_map:
         "192.168.100.1": ["1.2.3.4:4242"]
     #+END_SRC

     lighthouse 节点上，设置 =am_lighthouse: true=,以及 =lighthouse.hosts= 留空。

     其他节点上: =lighthouse.hosts= 字段里填上 lighthouse 节点的组网 IP（即 192.168.100.1）。

     接着把配置文件拷贝到各个节点的 =/etc/nebula/config.yaml=

**** 运行
     nebula 进程没有自带 daemon 模式，就用 supervisor 来运行吧。
     #+BEGIN_SRC bash
     apt-get install supervisor
     cat <<EOF >/etc/supervisor/conf.d/nebula.conf
     [program:nebula]
     command=/usr/local/bin/nebula -config /etc/nebula/config.yaml
     EOF
     supervisorctl reload
     #+END_SRC

     可以再用 =supervisorctl status= 看下状态。


*** 部署 kubernetes
    大体跟上一篇类似。不过这次用 [[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/][calico]] 作 network add-on，因为我真的养了一只 calico-cat。

**** 配置 calico
     kubeadm 提供的 calico quickstart 命令需要调整，要把 [[https://docs.projectcalico.org/v3.8/manifests/calico.yaml][calico.yaml]] 下载到机器上修改后使用：
     - calico 默认使用了 =192.168.0.0/16= 网段，与 nebula 使用的网段重复，我将文件里的 CALICO_IPV4POOL_CIDR 修改成 =192.168.128.0/24=.
     - 默认的 autodetection_method 是 first-found，这样可能用不到 nebula 的 IP 地址，文件里没有这个字段，需要新加，修改后长这样：
       #+BEGIN_SRC yaml
       - name: IP
         value: "autodetect"
       - name: IP_AUTODETECTION_METHOD
        value: "interface=nebula*"
       #+END_SRC


     在 =kubeadm init= 之后，执行 =kubectl apply -f calico.yaml= 即可。

**** 调整 kubelet 的 node-ip
     在每个节点都完成了 =kubeadm join= 命令之后，需要调整下节点上 kubelet 的 node-ip 参数，修改为 nebula 的 IP 地址,以使 =kubectl logs= 之类的命令可以正常工作。

     一键脚本,注意如果改了 nebula 的网卡名设置，脚本里也要对应的修改:
     #+BEGIN_SRC bash
     sed -i s,'KUBELET_KUBEADM_ARGS="[^"]*',"& --node-ip=$(ip addr show nebula1 | grep -Po 'inet \K[\d.]+')", /var/lib/kubelet/kubeadm-flags.env
     systemctl restart kubelet
     systemctl restart docker
     #+END_SRC

     在控制节点上执行 =kubectl -n kube-system get pods -o wide=,看 IP 那一列，如果显示 nebula 的地址，就表示成功了。
** DONE Org-mode 导出中文 PDF                                                   :@Post:emacs:latex:pdf:
   CLOSED: [2020-04-20 Mon 18:50]
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: export-org-mode-in-chinese-to-pdf-with-custom-latex-class
   :EXPORT_FILE_NAME: index.md
   :EXPORT_DATE: [2020-04-20 Mon 18:14]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:

目前在用的 emacs 配置基于 [[https://github.com/redguardtoo/emacs.d][redguardtoo/emacs.d]] 修改而来。

配置的默认功能有：

- 使用 [[https://github.com/ElegantLaTeX/ElegantPaper][ElegantLaTeX/ElegantPaper]] 作为默认的导出模板;
- 使用 [[https://github.com/gpoore/minted][gpoore/minted]] 进行代码高亮;
- 使用 ctex 默认的字体设置;

下载 [[https://github.com/ElegantLaTeX/ElegantPaper/blob/master/elegantpaper.cls][elegantpaper.cls]] 放到 =org= 文档同级目录内。

在 =~/.custom.el= 里添加配置：

#+BEGIN_SRC lisp
(with-eval-after-load 'ox-latex
 ;; http://orgmode.org/worg/org-faq.html#using-xelatex-for-pdf-export
 ;; latexmk runs pdflatex/xelatex (whatever is specified) multiple times
 ;; automatically to resolve the cross-references.
 (setq org-latex-pdf-process '("latexmk -xelatex -quiet -shell-escape -f %f"))
 (add-to-list 'org-latex-classes
               '("elegantpaper"
                 "\\documentclass[lang=cn]{elegantpaper}
                 [NO-DEFAULT-PACKAGES]
                 [PACKAGES]
                 [EXTRA]"
                 ("\\section{%s}" . "\\section*{%s}")
                 ("\\subsection{%s}" . "\\subsection*{%s}")
                 ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
                 ("\\paragraph{%s}" . "\\paragraph*{%s}")
                 ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
  (setq org-latex-listings 'minted)
  (add-to-list 'org-latex-packages-alist '("" "minted")))
#+END_SRC

在 =org= 文档的头部添加参数：

#+BEGIN_SRC org
#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: elegantpaper
#+OPTIONS: prop:t
#+END_SRC


安装 =minted= 的依赖:

#+BEGIN_SRC bash
brew install pygments
#+END_SRC

之后将光标移动到要导出的 Subtree, =C-c C-e C-s l p= 即可。

**参考**

- http://orgmode.org/worg/org-faq.html#using-xelatex-for-pdf-export
- https://orgmode.org/manual/Export-Settings.html#Export-settings
** TODO Configure Debian box as router                                          :@Post:debian:router:network:
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: configure-debian-box-as-router
   :EXPORT_FILE_NAME: index
   :EXPORT_DATE: [2020-04-14 Tue 16:36]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
** DONE 使用 docker 部署一个生产环境可用的 ELK 集群                             :@Post:elasticsearch:docker:tls:
   CLOSED: [2020-05-27 Wed 22:58]
   :PROPERTIES:
   :EXPORT_HUGO_BUNDLE: deploy-production-ready-elk-cluster-with-docker
   :EXPORT_HUGO_PAIRED_SHORTCODES: %notice expand gallery
   :EXPORT_FILE_NAME: index.md
   :EXPORT_DATE: [2020-05-19 Tue 17:03]
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :toc true
   :END:
   
近日在网上冲浪的时候，看到 soulteary 同学撰写的 [[https://soulteary.com/2020/05/04/use-docker-to-build-elk-environment.html][使用 Docker 搭建 ELK 环境]]，发现文中提及的 [[https://github.com/deviantony/docker-elk][docker-elk]] 项目. 
与我自行维护用于部署生产环境 ELK 集群的项目, 在代码结构上十分相似。便将本地项目迁移到 docker-elk, 并向上游项目发了一个针对开启 TLS 的 [[https://github.com/deviantony/docker-elk/pulls][Patch]].
顺便针对在生产环境部署多主机节点集群、开启 TLS 加密通信做一个详细的介绍。

本文将会涉及到：

- 基于 [[https://github.com/deviantony/docker-elk][docker-elk]] 项目进行部署操作，避免重复造轮子
- 用三个主机节点组成 elasticsearch 集群
- 集群内部使用自签名的 TLS 证书通信
- kibana, logstash 与 elasticsearch 集群的通信开启 TLS 加密

*** 准备工作
**** 熟悉 docker 和 docker-elk 项目
后续假设读者已经熟悉:

- docker/docker-compose 的基本用法
- docker-elk 项目的用法
- elasticsearch 的基本概念

如果还不熟悉，可以跟着开头提及的文章操作一次。
**** 节点规划
***** 节点
 本文将使用三个主机节点部署 elasticsearch 集群, 三个节点在同一个内网中，并且可以互相连接:

 - 节点A: 主机名: =es01.yuchi.lab=, IP: =10.11.12.13=
 - 节点B: 主机名: =es02.yuchi.lab=, IP: =10.11.12.14=
 - 节点C: 主机名: =es03.yuchi.lab=, IP: =10.11.12.15=

备注:

- 节点A作为 master 节点使用
- 主机名将作为环境变量 NODE_NAME，在后文使用

***** 端口 
所有 elasticsearch 实例，在主机上暴露的端口修改为:

- http: 9220(默认 9200)
- tcp: 9320(默认 9300)

以方便在已经部署了 elasticsearch 示例的设备上测试。
***** 数据目录
- es 数据存储在 `/data/es/${NODE_NAME}-data/` 目录
- es 日志存储在 `/data/es/${NODE_NAME}-log/` 目录

可以在各个节点上保存并执行下面的脚本:

#+NAME: prepare_dir.sh
#+BEGIN_SRC bash
#!/bin/bash

if [[ "x$1" == "x" ]]; then
    echo "need param as NODE_NAME"
    exit 1
fi
export NODE_NAME=$1

dataDir="/data/es/${NODE_NAME}-data/"
logDir="/data/es/${NODE_NAME}-log/"

sudo mkdir $dataDir -p
sudo mkdir $logDir -p

sudo chmod g+rwx $dataDir
sudo chmod g+rwx $logDir

sudo chgrp 1000 $dataDir
sudo chgrp 1000 $logDir
#+END_SRC

然后执行(替换 NODE_NAME 为真实的主机名): 
#+BEGIN_SRC bash
sudo bash prepare_dir.sh NODE_NAME
#+END_SRC

*** 准备配置
**** 准备自签名证书
生成证书在本地执行.

#+attr_shortcode: warning
#+begin_notice
不要将证书文件添加到 git 仓库中。 
#+end_notice

***** 修改配置文件
 进入本地的 docker-elk 目录，添加下面的文件:

 #+NAME: create_cert.yml
 #+BEGIN_SRC yaml
 version: "3.2"

 services:
   create_ca:
     container_name: create_ca
     image: docker.elastic.co/elasticsearch/elasticsearch:${ELK_VERSION}
     command: >
       bash -c '
         yum install -y -q -e 0 unzip;
         if [[ ! -f /certs/ca.zip ]]; then
           bin/elasticsearch-certutil ca --ca-dn ${CA_DN} --days ${CA_DAYS} --pass ${CA_PASSWORD} --pem --out /certs/ca.zip;
           unzip /certs/ca.zip -d /certs;
         fi;
         chown -R 1000:0 /certs
       '
     user: "0"
     working_dir: /usr/share/elasticsearch
     volumes:
       [
         "./tls/certs:/certs",
         "./tls:/usr/share/elasticsearch/config/certificates",
       ]

   create_certs:
     container_name: create_certs
     image: docker.elastic.co/elasticsearch/elasticsearch:${ELK_VERSION}
     command: >
       bash -c '
         yum install -y -q -e 0 unzip;
         if [[ ! -f /certs/bundle.zip ]]; then
           bin/elasticsearch-certutil cert --ca-cert /certs/ca/ca.crt --ca-key /certs/ca/ca.key --ca-pass ${CA_PASSWORD} --pem --in config/certificates/instances.yml -out /certs/bundle.zip;
           unzip /certs/bundle.zip -d /certs;
         fi;
         chown -R 1000:0 /certs
       '
     user: "0"
     working_dir: /usr/share/elasticsearch
     volumes:
       [
         "./tls/certs:/certs",
         "./tls:/usr/share/elasticsearch/config/certificates",
       ]
 #+END_SRC

 #+NAME: tls/instances.yml
 #+BEGIN_SRC yaml
 instances:
   - name: es01.yuchi.lab
     dns:
       - es01.yuchi.lab
       - localhost
       - es01
     ip:
       - 127.0.0.1
       - 10.11.12.13
   - name: es02.yuchi.lab
     dns:
       - es02.yuchi.lab
       - localhost
       - es02
     ip:
       - 127.0.0.1
       - 10.11.12.14
   - name: es03.yuchi.lab
     dns:
       - es03.yuchi.lab
       - localhost
       - es03
     ip:
       - 127.0.0.1
       - 10.11.12.15
 #+END_SRC

 在 .env 文件中添加 CA 证书相关的信息:
 #+BEGIN_SRC plaintext
 # self-sign tls
 CA_PASSWORD=ChangeMe
 CA_DN="CN=Elastic Certificate Tool Autogenerated CA"
 CA_DAYS=3650
 #+END_SRC

***** 生成证书

先执行命令生成 CA:
#+BEGIN_SRC bash
docker-compose -f create_cert.yml run --rm create_ca
#+END_SRC

会在 =tls/certs/= 目录下生成 CA 文件：

#+BEGIN_SRC plaintext
tls/certs/
├── ca
│   ├── ca.crt
│   └── ca.key
├── ca.zip
#+END_SRC

再使用 =tls/instances.yml= 文件生成每个节点的证书:

#+BEGIN_SRC bash
docker-compose -f create_cert.yml run --rm create_certs
#+END_SRC

最终 =tls/certs/= 目录里的文件结构如下:

#+BEGIN_SRC plaintext 
tls/certs/
├── bundle.zip
├── ca
│   ├── ca.crt
│   └── ca.key
├── ca.zip
├── es01.yuchi.lab
│   ├── es01.yuchi.lab.crt
│   └── es01.yuchi.lab.key
├── es02.yuchi.lab
│   ├── es02.yuchi.lab.crt
│   └── es02.yuchi.lab.key
└── es03.yuchi.lab
    ├── es03.yuchi.lab.crt
    └── es03.yuchi.lab.key
#+END_SRC

**** 更新 docker-compose 配置
***** volume 配置
如开头所说，我们会将数据和日志保存在主机上的 =/data/es/${NODE_NAME}-{data,log}= 目录中，需要更新 =docker-compose.yml= 中的 =volumes= 部分为:
#+BEGIN_SRC yaml
volumes:
  esdata01:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: '/data/es/${NODE_NAME}-data/'
  eslog01:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: '/data/es/${NODE_NAME}-log/'
#+END_SRC

在 =services.elasticsearch.volumes= 中添加配置，高亮的配置在原文件中存在，修改即可:
#+BEGIN_SRC yaml :hl_lines 5-7
services:
  elasticsearch:
    ...
    volumes:
      - type: volume
        source: esdata01
        target: /usr/share/elasticsearch/data
      - type: volume
        source: eslog01
        target: /usr/share/elasticsearch/logs
#+END_SRC
***** 证书配置
在 =.env= 文件中添加变量，设置容器内的证书路径(让 docker-compose 显得更整洁):
#+BEGIN_SRC plaintext
CERTS_DIR=/usr/share/elasticsearch/config/certificates
#+END_SRC

在 =services.elasticsearch.volumes= 中添加配置，挂载证书目录:
#+BEGIN_SRC yaml
sevices:
  elasticsearch:
    ...
    volumes:
      - type: bind
        source: ./tls/certs
        target: $CERTS_DIR
        read_only: true
#+END_SRC

在 =services.elasticsearch.environment= 中添加配置，启用证书:
#+BEGIN_SRC yaml
sevices:
  elasticsearch:
    ...
    environment:
      xpack.security.enabled: "true"
      xpack.security.http.ssl.enabled: "true"
      xpack.security.http.ssl.key: ${CERTS_DIR}/${NODE_NAME}/${NODE_NAME}.key
      xpack.security.http.ssl.certificate_authorities: ${CERTS_DIR}/ca/ca.crt
      xpack.security.http.ssl.certificate: ${CERTS_DIR}/${NODE_NAME}/${NODE_NAME}.crt
      xpack.security.transport.ssl.enabled: "true"
      xpack.security.transport.ssl.verification_mode: certificate
      xpack.security.transport.ssl.certificate: ${CERTS_DIR}/${NODE_NAME}/${NODE_NAME}.crt
      xpack.security.transport.ssl.certificate_authorities: ${CERTS_DIR}/ca/ca.crt
      xpack.security.transport.ssl.key: ${CERTS_DIR}/${NODE_NAME}/${NODE_NAME}.key
#+END_SRC
***** 节点和集群配置
继续在 =services.elasticsearch.environment= 中添加配置:
#+BEGIN_SRC yaml :hl_lines 7,9,11,12-13
sevices:
  elasticsearch:
    ...
    environment:
      node.name: ${NODE_NAME}
      http.port: 9200
      http.publish_port: 9220
      network.host: 0.0.0.0
      network.publish_host: ${NODE_IP}
      transport.tcp.port: 9300
      transport.publish_port: 9320
      cluster.initial_master_nodes: "es01.yuchi.lab:9320"
      discovery.seed_hosts: "es01.yuchi.lab:9320"

      xpack.security.enabled: "true"
      ...
#+END_SRC

一些解释：
- =publish_port=: 当 elastcisearch 进程监听的端口，与实际被连接的端口不一致时，需要设置 publish_port 配置，比如容器内监听了 9200，主机上公布了 9220，集群内的其他节点就会通过 9220 进行连接
- =publish_host=: 同样适用于容器场景，集群在节点发现时，种子节点会通知其他节点，通过 publish_host 设置的 IP 进行连接
- =initial_master_nodes=, =seed_hosts= 都可以使用逗号分割，设置多个

同时，删除这一行:
#+BEGIN_SRC yaml :hl_lines 5,
sevices:
  elasticsearch:
    ...
    environment:
      discovery.type: single-node
#+END_SRC
***** 调整系统参数
内存、ulimit 相关参数直接修改 =docker-compose.yml= 即可, JVM 内存的具体数值需要根据主机的参数、业务需求和部署规划决定:

#+BEGIN_SRC yaml
services:
  elasticsearch:
  ...
    environment:
      bootstrap.memory_lock: "true"
      ES_JAVA_OPTS: "-Xmx2g -Xms2g"
    ulimits:
      memlock:
        soft: -1
        hard: -1
#+END_SRC

=vm.max_map_count= 需要在主机上修改, 同时，把这一行写入 =/etc/sysctl.conf=:

#+BEGIN_SRC bash
sysctl -w vm.max_map_count=262144
#+END_SRC

***** 同步配置到各个节点
将 docker-elk 项目同步到每个节点上。

#+BEGIN_SRC bash
rsync -aPe \
'ssh -p{your-ssh-port}' \
--exclude {'tls/certs/ca.zip', 'tls/certs/bundle.zip'} \
../docker-elk/ es01.yuchi.lab:~/docker-elk
#+END_SRC

本文为了方便操作，直接使用 rsync 将整个目录拷贝到目标节点上，在实际生产环境中，可以尝试使用 ansible 等工具进行自动化操作。同时，也可以使用 ansible-vault 加密证书文件，避免明文存储和拷贝。

*** 启动集群
**** 启动 master 节点
前面指定了 =es01.yuchi.lab= 作为集群的主节点，先启动这个节点上的服务。ssh 登录到节点上，进入 =~/docker-elk= 目录，执行:
#+BEGIN_SRC bash
NODE_NAME=es01.yuchi.lab NODE_IP=10.11.12.13 sudo -E docker-compose -f docker-compose.yml up elasticsearch
#+END_SRC

终端上的输出，注意观察高亮行的输出内容，表示服务启动成功:
#+BEGIN_SRC plaintext :hl_lines 7,
Creating volume "docker-elk_esdata01" with local driver
Creating volume "docker-elk_eslog01" with local driver
Creating docker-elk_elasticsearch_1 ... done
Attaching to docker-elk_elasticsearch_1
elasticsearch_1  | Created elasticsearch keystore in /usr/share/elasticsearch/config
...
elasticsearch_1  | {"type": "server", "timestamp": "2020-05-25T05:24:25,947Z", "level": "INFO", "component": "o.e.c.r.a.AllocationService", "cluster.name": "docker-cluster", "node.name": "es01.yuchi.lab", "message": "Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.watcher-history-10-2020.05.25][0]]]).", "cluster.uuid": "uNb6qD9kStSiwxPIi9CvUw", "node.id": "ofLOlsGZT_OgBfJVyr1I0Q"  }
#+END_SRC

接下来，在主机上的另一个终端内，用 =curl= 检查节点的状态:
#+BEGIN_SRC bash :hl_lines 4,
curl -u elastic:changeme --cacert tls/certs/ca/ca.crt 'https://es01.yuchi.lab:9220/_cluster/health?pretty=true'
{
  "cluster_name" : "docker-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 5,
  "active_shards" : 5,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}

#+END_SRC
**** 启动数据节点(从节点)
在另外两个主机上，分别进入 =docker-elk= 目录，再执行:
#+BEGIN_SRC bash
# on es02
NODE_NAME=es02.yuchi.lab NODE_IP=10.11.12.14 sudo -E docker-compose -f docker-compose.tls.yml up elasticsearch
# on es03
NODE_NAME=es03.yuchi.lab NODE_IP=10.11.12.15 sudo -E docker-compose -f docker-compose.tls.yml up elasticsearch
#+END_SRC

终端上会输出类似下面的日志：
#+BEGIN_SRC plaintext :hl_lines 3,
...
elasticsearch_1  | {"type": "server", "timestamp": "2020-05-26T02:27:45,263Z", "level": "INFO", "component": "o.e.b.BootstrapChecks", "cluster.name": "docker-cluster", "node.name": "es02.yuchi.lab", "message": "bound or publishing to a non-loopback address, enforcing bootstrap checks" }
elasticsearch_1  | {"type": "server", "timestamp": "2020-05-26T02:27:45,274Z", "level": "INFO", "component": "o.e.c.c.ClusterBootstrapService", "cluster.name": "docker-cluster", "node.name": "es02.yuchi.lab", "message": "skipping cluster bootstrapping as local node does not match bootstrap requirements: [es01.yuchi.lab]" }
elasticsearch_1  | {"type": "server", "timestamp": "2020-05-26T02:27:46,322Z", "level": "INFO", "component": "o.e.c.s.ClusterApplierService", "cluster.name": "docker-cluster", "node.name": "es02.yuchi.lab", "message": "master node changed {previous [], current [{es01.yuchi.lab}{ofLOlsGZT_OgBfJVyr1I0Q}{2vdYT2RGT92uqH4v8S3duQ}{10.11.12.13}{10.11.12.13:9320}{dilm}{ml.machine_memory=201400594432, ml.max_open_jobs=20, xpack.installed=true}]}, added {{es01.yuchi.lab}{ofLOlsGZT_OgBfJVyr1I0Q}{2vdYT2RGT92uqH4v8S3duQ}{10.11.12.13}{10.11.12.13:9320}{dilm}{ml.machine_memory=201400594432, ml.max_open_jobs=20, xpack.installed=true}}, term: 4, version: 80, reason: ApplyCommitRequest{term=4, version=80, sourceNode={es01.yuchi.lab}{ofLOlsGZT_OgBfJVyr1I0Q}{2vdYT2RGT92uqH4v8S3duQ}{10.11.12.13}{10.11.12.13:9320}{dilm}{ml.machine_memory=201400594432, ml.max_open_jobs=20, xpack.installed=true}}" }
...
#+END_SRC

此时再次检查集群状态,可以看到节点数量已经变成三个：
#+BEGIN_SRC bash :hl_lines 6-7
curl -u elastic:changeme --cacert tls/certs/ca/ca.crt 'https://es01.yuchi.lab:9220/_cluster/health?pretty=true'
{
  "cluster_name" : "docker-cluster",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 8,
  "active_shards" : 16,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 100.0
}
#+END_SRC
**** 重置内建账户密码 
在 master 节点所在主机上执行命令:

#+BEGIN_SRC bash 
NODE_NAME=es01.yuchi.lab NODE_IP=10.11.12.13 \
sudo -E docker-compose -f docker-compose.yml \
exec -T elasticsearch bin/elasticsearch-setup-passwords auto \
--batch --url https://localhost:9200
#+END_SRC

#+BEGIN_SRC plaintext
Changed password for user apm_system
PASSWORD apm_system = 4QHu6U58oH3Uz9dEYbQa

Changed password for user kibana
PASSWORD kibana = rGPTCtn2B3uUJbyMc79y

Changed password for user logstash_system
PASSWORD logstash_system = DzNT28wMiPjAOPuSQUBV

Changed password for user beats_system
PASSWORD beats_system = 8xHjc6eMIIDP8dj3KLVm

Changed password for user remote_monitoring_user
PASSWORD remote_monitoring_user = nvgD7uPvHiy86MkAEiPe

Changed password for user elastic
PASSWORD elastic = l6yM662rGcKKoA3lOxSM
#+END_SRC

保存好输出的结果.并修改下面三个文件中的 elastic 密码，为后面启动 kibana,logstash 做准备:

- kibana/config/kibana.yml
- logstash/config/logstash.yml
- logstash/pipeline/logstash.conf
**** 启动 kibana
修改 =docker-compose.yml= 中的 =services.kibana= 部分为:
#+BEGIN_SRC yaml :hl_lines 11-14,15-17,22-23
  kibana:
    build:
      context: kibana/
      args:
        ELK_VERSION: $ELK_VERSION
    volumes:
      - type: bind
        source: ./kibana/config/kibana.yml
        target: /usr/share/kibana/config/kibana.yml
        read_only: true
      - type: bind
        source: ./tls/certs
        target: $CERTS_DIR
        read_only: true
    environment:
      ELASTICSEARCH_HOSTS: "https://es01.yuchi.lab:9220"
      ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES: "${CERTS_DIR}/ca/ca.crt"
    ports:
      - "5600:5600"
    networks:
      - elk
    depends_on:
      - elasticsearch
#+END_SRC

- 11-14 行将证书挂载到容器里
- 15-17 行指定 elasticsearch 地址使用 HTTPS 协议，并指定 CA 证书的路径
  - =ELASTICSEARCH_HOSTS= 可以使用逗号分割写多个地址
  - 如果 elasticsearch 与 kibana 不运行在同一台主机上，需要删除 22-23 行

然后到 master 节点所在主机上执行:
#+BEGIN_SRC bash
NODE_NAME=es01.yuchi.lab NODE_IP=10.11.12.13 \
sudo -E docker-compose -f docker-compose.yml up --build kibana
#+END_SRC

启动成功时，终端里输出:
#+BEGIN_SRC plaintext
...
kibana_1         | {"type":"log","@timestamp":"2020-05-27T12:11:20Z","tags":["info","http","server","Kibana"],"pid":6,"message":"http server running at http://0:5601"}
...
#+END_SRC
**** 启动 logstash
修改 =logstash/config/logstash.yaml=:
#+BEGIN_SRC yaml
# 修改 ES 地址
xpack.monitoring.elasticsearch.hosts: [ "https://es01.yuchi.lab:9220" ]

# 添加 CA 证书配置
xpack.monitoring.elasticsearch.ssl.certificate_authority: "${LS_CACERT_FILE}"
#+END_SRC

修改 =logstash/pipeline/logstash.conf= 中的 output 部分为: 
#+BEGIN_SRC plaintext
output {
	elasticsearch {
		hosts => "https://es01.yuchi.lab:9220"
		cacert => "${LS_CACERT_FILE}"
		user => "elastic"
		password => "real-password"
	}
}
#+END_SRC

修改 =docker-compose.yml= 中的 =services.logstash= (高亮的部分):
#+BEGIN_SRC yaml :hl_lines 4-7,10
  logstash:
    volumes:
      ...
      - type: bind
        source: ./tls/certs
        target: $CERTS_DIR
        read_only: true
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
      LS_CACERT_FILE: "${CERTS_DIR}/ca/ca.crt"
    ...
#+END_SRC

再执行启动命令即可:
#+BEGIN_SRC bash
NODE_NAME=es01.yuchi.lab NODE_IP=10.11.12.13 \
sudo -E docker-compose -f docker-compose.yml up --build logstash
#+END_SRC

**** 注入测试数据
 把 =/var/log/kern.log= 通过 logstash 注入到集群中:
 #+BEGIN_SRC bash
 sudo cat /var/log/kern.log | nc -c localhost 5000
 #+END_SRC

 登录 kibana,在 =Settings.Kibana.Index Patterns= 页面，新建 =logstash*= pattern, 再回到 kibana 首页就可以看到数据了。

 #+begin_gallery
 #+attr_html: :alt kibana create index pattern
 [[file:images/es/kibana-create-pattern.png]] 

 #+attr_html: :alt kibana data
 [[file:images/es/kibana-data.png]]

 #+end_gallery

*** 完整配置
上述修改后的完整配置放在 [[https://github.com/wych42/docker-elk/tree/prod-demo][Github]].

以上。